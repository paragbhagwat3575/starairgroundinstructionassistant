<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Star Air Ground Staff Asisstant</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Arial; }
    body { margin: 24px; }
    .row { display:flex; gap:12px; align-items:center; flex-wrap:wrap; }
    button { padding:10px 14px; border-radius:10px; border:1px solid #ccc; cursor:pointer; }
    select, input { padding:8px 10px; border-radius:8px; border:1px solid #ccc; }
    #log { white-space: pre-wrap; background:#0b1020; color:#cde1ff; padding:12px; border-radius:10px; min-height:160px; }
    #transcript { min-height: 48px; }
    .pill { padding:4px 8px; border-radius:999px; background:#eef2ff; border:1px solid #ccd4ff; }
    .ok { color:#046a33; }
    .err{ color:#b00020; }
  </style>
</head>
<body>
  <h2>Star Air Ground Staff Asisstant</h2>

  <div class="row" style="margin-bottom:12px">
    <label>Model:
      <select id="model">
        <option value="gpt-realtime">gpt-realtime</option>
      </select>
    </label>
    <label>Voice:
      <select id="voice">
        <option value="marin">marin</option>
        <option value="alloy">alloy</option>
        <option value="verse">verse</option>
      </select>
    </label>
    <span class="pill">Token: <span id="tokenStatus">not requested</span></span>
    <span class="pill">PC: <span id="pcState">new</span></span>
    <span class="pill">DC: <span id="dcState">closed</span></span>
  </div>

  <div class="row" style="margin-bottom:12px">
    <button id="connect">Connect & Start Mic</button>
    <button id="end" disabled>End Session</button>
  </div>

  <p><b>Live Text</b> (ASR + assistant):</p>
  <div id="transcript"></div>

  <p style="margin-top:18px"><b>Event Log</b></p>
  <div id="log"></div>

  <!-- Remote audio from the model -->
  <audio id="remoteAudio" autoplay></audio>

  <script>
    // ==== CONFIG: your endpoints ====
    const TOKEN_ENDPOINT = "https://getrealtimeusecase3-319132888215.asia-south1.run.app";
    const TOOL_ENDPOINT  = "https://getrealtimebackendanswer-319132888215.asia-south1.run.app";
    // ================================

    const $ = (id) => document.getElementById(id);
    const tokenStatus = $("tokenStatus");
    const pcState     = $("pcState");
    const dcState     = $("dcState");
    const logEl       = $("log");
    const transcriptEl= $("transcript");
    const remoteAudio = $("remoteAudio");

    let pc, dc, micStream;
    let sessionModel, sessionVoice;
    let ephemeralKey = null;

    // buffer for displayed transcript
    let outputText = "";
    let waitingForTool = false;


    // --- convenience logging
    function log(msg, level="") {
      const ts = new Date().toLocaleTimeString();
      logEl.textContent += `[${ts}] ${level?`[${level}] `:""}${msg}\n`;
      logEl.scrollTop = logEl.scrollHeight;
    }

    function setStates() {
      tokenStatus.textContent = ephemeralKey ? "received" : "not requested";
      dcState.textContent     = dc?.readyState || "closed";
      pcState.textContent     = pc?.connectionState || "new";
    }

    // Try to extract ek_* from several possible shapes
    function extractEphemeralKey(json) {
      if (!json) return null;
      if (typeof json.value === "string") return json.value;
      if (json.client_secret?.value) return json.client_secret.value;
      if (json.clientSecret?.value)  return json.clientSecret.value;
      if (json.secret?.value)        return json.secret.value;
      return null;
    }

    async function getClientSecret() {
      sessionModel = $("model").value;
      sessionVoice = $("voice").value;

      const body = {
        model: sessionModel,
        voice: sessionVoice,
        session_type: "realtime"
        // If you want to override instructions from the backend, add "instructions" here.
      };

      const res  = await fetch(TOKEN_ENDPOINT, {
        method: "POST",
        headers: { "Content-Type":"application/json" },
        body: JSON.stringify(body),
      });
      const json = await res.json();
      log(`Token endpoint status: ${res.status}`);
      log(JSON.stringify(json, null, 2));
      const ek = extractEphemeralKey(json);
      if (!ek) throw new Error("Could not find ephemeral key in response");
      return ek;
    }

    function attachMic(pc) {
      return navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
        micStream = stream;
        stream.getTracks().forEach(t => pc.addTrack(t, stream));
        log("Mic attached ✔️", "ok");
      });
    }

    function setMicEnabled(enabled) {
      if (!micStream) return;
      micStream.getAudioTracks().forEach(t => t.enabled = enabled);
    }

    function setupPeer() {
      pc = new RTCPeerConnection();
      setStates();

      pc.onconnectionstatechange = () => {
        setStates();
        log(`PeerConnection: ${pc.connectionState}`);
      };

      pc.ontrack = (e) => {
        remoteAudio.srcObject = e.streams[0];
        log("Got remote audio track");
      };

      // Create our outgoing data channel for events
      dc = pc.createDataChannel("oai-events");
      dc.onopen = () => {
        setStates();
        log("DataChannel open ✔️", "ok");

        // (A) prevent TTS from being interrupted by input noise
        dc.send(JSON.stringify({
          type: "session.update",
          session: { audio: { input: { turn_detection: { interrupt_response: false } } } }
        }));

        // (B) also ask for text deltas in addition to audio (handy for UI/debug)
        dc.send(JSON.stringify({
          type: "session.update",
          session: { output_modalities: ["audio","text"] }
        }));
      };
      dc.onclose = () => { setStates(); log("DataChannel closed"); };
      dc.onerror = (e) => { log(`DC error: ${e.message || e}`, "err"); };

      dc.onmessage = (evt) => {
        try { handleServerEvent(JSON.parse(evt.data)); }
        catch (e) { log(`DC message (non-JSON): ${evt.data}`); }
      };

      // Also capture any incoming data channels (defensive)
      pc.ondatachannel = (evt) => {
        if (!dc || dc.readyState !== "open") {
          dc = evt.channel;
          dc.onmessage = (e) => {
            try { handleServerEvent(JSON.parse(e.data)); }
            catch { log(`DC message (non-JSON): ${e.data}`); }
          };
          dc.onopen = () => { setStates(); log("Incoming DataChannel open ✔️", "ok"); };
          dc.onclose = () => { setStates(); log("Incoming DataChannel closed"); };
        }
      };

      // Add a recv transceiver to ensure we get audio downlink
      pc.addTransceiver("audio", { direction: "sendrecv" });
    }

    async function startSession() {
      $("connect").disabled = true;
      $("end").disabled = true;

      try {
        // 1) get ephemeral key from your backend
        ephemeralKey = await getClientSecret();
        setStates();

        // 2) build PC + get mic
        setupPeer();
        await attachMic(pc);

        // 3) create offer SDP
        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);

        // 4) POST offer.sdp to OpenAI Realtime to get answer.sdp
        // 4) POST offer.sdp to OpenAI Realtime to get answer.sdp
		const url = `https://api.openai.com/v1/realtime/calls?model=${encodeURIComponent(sessionModel)}`;
		const sdpRes = await fetch(url, {
		  method: "POST",
		  body: offer.sdp,
		  headers: {
		    Authorization: `Bearer ${ephemeralKey}`,
		    "Content-Type": "application/sdp"
		  }
		});

		const body = await sdpRes.text();

		// Treat any non-2xx as failure
		if (!sdpRes.ok) {
		  log(`Realtime /calls HTTP ${sdpRes.status}:\n${body}`, "err");
		  throw new Error("Realtime /calls failed");
		}

		// If we didn’t get SDP, the body won’t start with “v=”
		if (!body.startsWith("v=")) {
		  log(`Realtime /calls returned non-SDP:\n${body}`, "err");
		  throw new Error("Realtime /calls did not return SDP");
		}

		await pc.setRemoteDescription({ type: "answer", sdp: body });
		log("WebRTC connected to Realtime ✔️", "ok");

        $("end").disabled = false;
      } catch (err) {
        log(`Start error: ${err.message || err}`, "err");
        $("connect").disabled = false;
      }
    }

    async function endSession() {
      try {
        if (dc && dc.readyState === "open") {
          dc.send(JSON.stringify({ type: "response.cancel" }));
        }
      } catch {}
      try { micStream?.getTracks().forEach(t => t.stop()); } catch {}
      try { pc?.close(); } catch {}
      dc = null; pc = null; micStream = null; ephemeralKey = null;
      setStates();
      $("connect").disabled = false;
      $("end").disabled = true;
      log("Session ended");
    }

    // === Server event handling (tolerant across GA/beta names) ===
    const pendingFuncArgs = {}; // call_id -> string builder


	function handleServerEvent(m) {
	  // --- Live transcripts (audio + text) ------------------------------------
	  if (m.type === "response.output_audio_transcript.delta" && typeof m.delta === "string") {
	    outputText += m.delta;
	    transcriptEl.textContent = outputText;
	    return;
	  }
	  if (m.type === "response.output_audio_transcript.done") {
	    outputText += "\n";
	    transcriptEl.textContent = outputText;
	    return;
	  }
	  if (m.type === "response.output_text.delta" && typeof m.delta === "string") {
	    outputText += m.delta;
	    transcriptEl.textContent = outputText;
	    return;
	  }
	  if (m.type === "response.output_text.done") {
	    outputText += "\n";
	    transcriptEl.textContent = outputText;
	    return;
	  }
	  // (legacy names)
	  if (m.type === "response.text.delta" && typeof m.text === "string") {
	    outputText += m.text;
	    transcriptEl.textContent = outputText;
	    return;
	  }
	  if (m.type === "response.text.done") {
	    outputText += "\n";
	    transcriptEl.textContent = outputText;
	    return;
	  }

	  // --- TTS playback hooks: pause/resume mic so VAD can't interrupt --------
	  if (m.type === "output_audio_buffer.started") {
	    setMicEnabled(false);
	    log("Mic paused during TTS");
	    return;
	  }
	  if (m.type === "output_audio_buffer.stopped") {
	    setMicEnabled(true);
	    log("Mic resumed after TTS");
	    return;
	  }

	  // --- User turn ended: if we're NOT waiting for a tool, ask model to reply
	  if (m.type === "input_audio_buffer.committed") {
	    if (!waitingForTool && dc?.readyState === "open") {
	      dc.send(JSON.stringify({ type: "response.create" }));
	    }
	    return;
	  }

	  // --- Tool calling --------------------------------------------------------
	  // Start of a tool call (args arriving): mark waiting
	  if (m.type === "response.function_call_arguments.delta") {
	    const id = m.call_id || m.response_id || "default";
	    pendingFuncArgs[id] = (pendingFuncArgs[id] || "") + (m.delta || "");
	    waitingForTool = true;
	    return;
	  }

	  // Some builds explicitly add a function_call item
	  if (m.type === "response.output_item.added" && m.item?.type === "function_call") {
	    waitingForTool = true;
	    // fall through; we’ll handle completion below
	  }

	  // Tool call completed (either GA done event or single-shot function_call)
	  if (m.type === "response.function_call_arguments.done" || looksLikeFunctionCall(m)) {
	    const call_id = m.call_id || m.id || m.response_id || "default";
	    const name = m.name || m.item?.name || m.function?.name || m.tool?.name;

	    let argsRaw =
	      m.arguments ||
	      m.item?.arguments ||
	      pendingFuncArgs[call_id] ||
	      "{}";

	    let args;
	    try { args = (typeof argsRaw === "string") ? JSON.parse(argsRaw) : argsRaw; }
	    catch { args = { user_query: String(argsRaw) }; }

	    log(`Tool call → ${name}(${JSON.stringify(args)})`);

	    if (name === "backend_answer") {
	      waitingForTool = true;              // stay guarded until backend replies
	      handleBackendAnswer(call_id, args); // this will clear the flag in finally
	    } else {
	      speakVerbatim("Sorry, I don’t have that tool available.");
	      waitingForTool = false;
	    }

	    delete pendingFuncArgs[call_id];
	    return;
	  }

	  // When our tool result is written into the conversation, we are done waiting
	  if (m.type === "conversation.item.added" && m.item?.type === "function_call_output") {
	    waitingForTool = false;
	    return;
	  }
	  if (m.type === "conversation.item.done" && m.item?.type === "function_call_output") {
	    waitingForTool = false;
	    return;
	  }

	  // --- Useful connection hints --------------------------------------------
	  if (m.type && (m.type.includes("session") || m.type.includes("output_audio"))) {
	    log(`Event: ${m.type}`);
	  }
	}

	function looksLikeFunctionCall(m) {
	  return (
	    typeof m?.type === "string" &&
	    (m.type.includes("function_call") || m.type.includes("tool_call")) &&
	    (m.name || m.item?.name)
	  );
	}


    function looksLikeFunctionCall(m) {
      return (
        typeof m?.type === "string" &&
        (m.type.includes("function_call") || m.type.includes("tool_call")) &&
        (m.name || m.item?.name)
      );
    }

    // --- Speak exactly the provided string (outside the conversation queue) ---
    function speakVerbatim(text) {
      if (dc?.readyState !== "open") return;
      const msg = {
        type: "response.create",
        response: {
          conversation: "none",
          instructions: `Speak this verbatim, with a natural tone and no additions: ${text}`
        }
      };
      log(`→ send: ${JSON.stringify(msg)}`);
      dc.send(JSON.stringify(msg));
    }

    // --- Call your Cloud Run tool and feed the result back to the session ---
    // Replace your handleBackendAnswer function with this:
	// Replace your handleBackendAnswer function with this:
	async function handleBackendAnswer(call_id, args) {
	  try {
	    const res = await fetch(TOOL_ENDPOINT, {
	      method: "POST",
	      headers: { "Content-Type": "application/json" },
	      body: JSON.stringify(args || {}),
	    });
	    const result = await res.json();
	    log(`Tool result: ${JSON.stringify(result)}`);

	    if (!dc || dc.readyState !== "open") return;

	    const text = result.answer_text || result.clarify_question || "I hear you. Please continue.";

	    // Complete the tool call with the backend response
	    dc.send(JSON.stringify({
	      type: "conversation.item.create",
	      item: {
	        type: "function_call_output",
	        call_id,
	        output: text
	      }
	    }));

	    // Force the AI to create a response based on the tool output
	    dc.send(JSON.stringify({
	      type: "response.create",
	      response: {
	        instructions: `The tool returned: "${text}". You must speak this text exactly as provided, word for word, with no modifications or additions.`
	      }
	    }));

	  } catch (e) {
	    log(`Tool error: ${e.message || e}`, "err");
	    if (dc?.readyState === "open") {
	      dc.send(JSON.stringify({
	        type: "conversation.item.create",
	        item: {
	          type: "function_call_output",
	          call_id,
	          output: "Sorry—my helper service failed. Please try again."
	        }
	      }));
	      dc.send(JSON.stringify({ type: "response.create" }));
	    }
	  } finally {
	    waitingForTool = false;
	  }
}
    // --- UI helpers for ad-hoc replies (uses GA shape) ---
    function respondWithText(text) {
      if (dc?.readyState !== "open") return;
      speakVerbatim(text);
    }

    // --- UI wiring
    $("connect").addEventListener("click", startSession);
    $("end").addEventListener("click", endSession);

    // Initial state
    setStates();
    log("Ready. Click “Connect & Start Mic”.");
  </script>
</body>
</html>
